{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OC IML Projet 5 : Catégorisez automatiquement des questions\n",
    "\n",
    "Stack Overflow est un site célèbre de questions-réponses liées au développement informatique\n",
    "développez *un système de suggestion de tag* pour le site. Celui-ci prendra la forme d’un algorithme de machine learning qui assigne automatiquement plusieurs tags pertinents à une question.\n",
    "\n",
    "\n",
    "Ce notebook contient : \n",
    "- API preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gregory/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gregory/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/gregory/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True, font_scale=1.33)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 25)\n",
    "\n",
    "import string\n",
    "from string import punctuation \n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import scipy.stats as st\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import model_selection\n",
    "from sklearn.externals import joblib\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source paths\n",
    "PATH_SOURCE_QUESTIONS = '../../data/QueryResults.csv' \n",
    "# export path\n",
    "PATH_EXPORT_FOLDER = '../../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dictionnary of translation to suppress ponctuation\n",
    "replace_punctuation = str.maketrans(string.punctuation,\n",
    "                                    ' '*len(string.punctuation))\n",
    "def cleaning_text(questions_curr):\n",
    "\n",
    "    # lower case\n",
    "    questions_curr = ' '.join([w.lower() for w in \\\n",
    "                               nltk.word_tokenize(questions_curr) \\\n",
    "                              if not w.lower() in list(sw)])\n",
    "    # delete newlines\n",
    "    questions_curr = re.sub(r'\\s+', ' ', questions_curr)\n",
    "    # delete single quotes\n",
    "    questions_curr = re.sub(r\"\\'\", \" \", questions_curr)\n",
    "    # delete tags\n",
    "    questions_curr = re.sub('<[^<]+?>',' ', questions_curr)\n",
    "    # delete numbers (forming group = word with only numbers \n",
    "    # example : delete \"123\" but not \"a123\")\n",
    "    questions_curr = re.sub(r'\\b\\d+\\b','', questions_curr) \n",
    "    # delete ponctuation (replace by space)\n",
    "    questions_curr = questions_curr.translate(replace_punctuation)\n",
    "\n",
    "    return questions_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    '''\n",
    "    Stem words in tokens.\n",
    "    and suppress word < 3 characters\n",
    "    '''\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        if re.match('[a-zA-Z0-9]{3,}',item):\n",
    "            stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def myTokenizer(text):\n",
    "    '''\n",
    "    Create tokens from text\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress models for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "myModel = open(PATH_EXPORT_FOLDER + \\\n",
    "    'model_RF_tags51_max_depthNone_max_features31_min_samples_split2_n_estimators25.pkl',\n",
    "    'rb')\n",
    "clf = joblib.load(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/mdl_cmp_RF_tags51_max_depthNone_max_features31_min_samples_split2_n_estimators25.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compress\n",
    "joblib.dump(clf, PATH_EXPORT_FOLDER + \\\n",
    "    'mdl_cmp_RF_tags51_max_depthNone_max_features31_min_samples_split2_n_estimators25.pkl',\n",
    "    compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from disk other useful tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CounterVectorizer\n",
    "tf_vectorizer_sup_1 = joblib.load(PATH_EXPORT_FOLDER + 'cvect_tags51.pkl')\n",
    "# TfidfTransformer \n",
    "tfidf_transformer_sup_1 = joblib.load(PATH_EXPORT_FOLDER + 'tfidf_tags51.pkl')\n",
    "# MultiLabelBinarizer\n",
    "mlb = joblib.load(PATH_EXPORT_FOLDER + 'mlb_tags51.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stopwords [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = [\"p\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Test Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quest = pd.read_csv(PATH_SOURCE_QUESTIONS, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Text:\n",
      " How to insert an entry to a table only if it does not exist <p>My table looks like this  on sql server</p>\n",
      "\n",
      "<pre><code>wordId     word      \n",
      "----------------\n",
      "1214       pen           \n",
      "1215       men    \n",
      "1216       cat  \n",
      "</code></pre>\n",
      "\n",
      "<p>WordId and word is being passed with the stored procedure and,I need to check on my stored procedure if the wordId already exists on the table or not, and only if the wordId doesn't exists I need to execute the insert statement.  </p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quest_text = df_quest[df_quest[\"Id\"] == 50000005][\"Title\"] + \" \" + \\\n",
    "    df_quest[df_quest[\"Id\"] == 50000005][\"Body\"]\n",
    "quest_text = quest_text.values[0]\n",
    "print(\"Question Text:\\n\", quest_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question cleaned:\n",
      " how to insert an entry to a table only if it does not exist   my table looks like this on sql server       wordid word                          pen  men  cat       wordid and word is being passed with the stored procedure and   i need to check on my stored procedure if the wordid already exists on the table or not   and only if the wordid does n t exists i need to execute the insert statement    \n"
     ]
    }
   ],
   "source": [
    "quest_text_cleaned = cleaning_text(quest_text)\n",
    "print(\"Question cleaned:\\n\", quest_text_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CounterVectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contVectValue = tf_vectorizer_sup_1.transform([quest_text_cleaned])\n",
    "contVectValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdfValue = tfidf_transformer_sup_1.transform(contVectValue)\n",
    "tfIdfValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y_pred = clf.predict(tfIdfValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sql', 'sql-server')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.inverse_transform(encoded_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
