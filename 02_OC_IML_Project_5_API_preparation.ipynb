{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OC IML Projet 5 : Catégorisez automatiquement des questions\n",
    "\n",
    "Stack Overflow est un site célèbre de questions-réponses liées au développement informatique\n",
    "développez *un système de suggestion de tag* pour le site. Celui-ci prendra la forme d’un algorithme de machine learning qui assigne automatiquement plusieurs tags pertinents à une question.\n",
    "\n",
    "\n",
    "Ce notebook contient : \n",
    "- API preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gregory/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gregory/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/gregory/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True, font_scale=1.33)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 25)\n",
    "\n",
    "import string\n",
    "from string import punctuation \n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import scipy.stats as st\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import model_selection\n",
    "from sklearn.externals import joblib\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "# import user module\n",
    "from my_text_utils import myTokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source paths\n",
    "PATH_SOURCE_QUESTIONS = '../../data/QueryResults.csv' \n",
    "# export path\n",
    "#PATH_EXPORT_FOLDER = '../../data/'\n",
    "PATH_EXPORT_FOLDER = 'data/'\n",
    "# model sup RF\n",
    "mdlFileName = 'model_RF_tags51_max_depthNone_max_features31_min_samples_split2_n_estimators25.pkl'\n",
    "# stop words\n",
    "stopWordsFileName = 'stop_words_sw.pkl'\n",
    "# count vectorizer\n",
    "countVectFileName = 'cvect_tags51.pkl'\n",
    "# tfidf\n",
    "tfidfFileName = 'tfidf_tags51.pkl'\n",
    "# MultiLabelBinarizer\n",
    "mlbFileName = 'mlb_tags51.pkl'\n",
    "# df Topics tags \n",
    "dfTopicsTagsFileName = 'df_topics_tags_top100.pkl'\n",
    "# mode unsup LDA\n",
    "mdlUnsupFileName = 'model_LDA__learning_decay0.7_max_iter20_n_components100.pkl'\n",
    "# count vectorizer unsup\n",
    "countVectUnsupFileName = \"cvect_lda.pkl\"\n",
    "# id question \n",
    "id_supOK = 50000005 # mdl sup ok\n",
    "id_supNOK = 50000074 # mdl sup NOK => unpervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dictionnary of translation to suppress ponctuation\n",
    "replace_punctuation = str.maketrans(string.punctuation,\n",
    "                                    ' '*len(string.punctuation))\n",
    "def cleaning_text(questions_curr):\n",
    "\n",
    "    # lower case\n",
    "    questions_curr = ' '.join([w.lower() for w in \\\n",
    "                               nltk.word_tokenize(questions_curr) \\\n",
    "                              if not w.lower() in list(sw)])\n",
    "    # delete newlines\n",
    "    questions_curr = re.sub(r'\\s+', ' ', questions_curr)\n",
    "    # delete single quotes\n",
    "    questions_curr = re.sub(r\"\\'\", \" \", questions_curr)\n",
    "    # delete tags\n",
    "    questions_curr = re.sub('<[^<]+?>',' ', questions_curr)\n",
    "    # delete numbers (forming group = word with only numbers \n",
    "    # example : delete \"123\" but not \"a123\")\n",
    "    questions_curr = re.sub(r'\\b\\d+\\b','', questions_curr) \n",
    "    # delete ponctuation (replace by space)\n",
    "    questions_curr = questions_curr.translate(replace_punctuation)\n",
    "\n",
    "    return questions_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tags_from_text(text, tf_vectorizer, lda_model, \n",
    "                        df_topics_tags, no_max=10):\n",
    "    '''\n",
    "    Predict tags from text using tf, lda and tags2topic table\n",
    "    \n",
    "    tf vectorizer, lda and table must be input.\n",
    "    '''\n",
    "    # clean the text\n",
    "    text_cleaned = cleaning_text(text)\n",
    "\n",
    "    # calculate feature from text with tf already fitted\n",
    "    feat_curr =  tf_vectorizer.transform([text_cleaned])\n",
    "\n",
    "    # calculate topic distrib with lda model already fitted\n",
    "    topic_distrib_pred = lda_model.transform(feat_curr)\n",
    "    \n",
    "    # find best topic from table df_topics_tags\n",
    "    return find_tags_from_dtopics(topic_distrib_pred, df_topics_tags, \n",
    "                                  no_max=no_max)\n",
    "\n",
    "def find_tags_from_dtopics(d_topics, df_topics_tags, no_max=10):\n",
    "    '''\n",
    "    Find best no_max Tags from Topics by giving Topic number as input.\n",
    "    (By default no_max = 10)\n",
    "    Uses table linking Tags & Topics \n",
    "    \n",
    "    inputs : \n",
    "    - d_topics : topics distribution from LDA\n",
    "    - df_topics_tags : table linking Tags to Topics (By default df_topics_tags)\n",
    "    - no_max : number of best Tags to output\n",
    "    \n",
    "    returns the list of no_max Topics numbers (int)\n",
    "    '''\n",
    "    \n",
    "    # multiply topics each columns of df_topics_tags by distrib dtopics vector:\n",
    "    arr_tags = df_topics_tags.values*d_topics # table (tags(row)*Topics(col))\n",
    "    # sum each row (by Tags)\n",
    "    sum_distrib_tags = arr_tags.sum(axis=1) # vector (n Tags)\n",
    "    # create dataframe to link with tags\n",
    "    df_sum_tags = pd.DataFrame(data=sum_distrib_tags, columns=[\"d_sum\"], \n",
    "                           index=df_topics_tags.index)\n",
    "    # return no_max Tags with best score\n",
    "    return list(df_sum_tags.sort_values(by=\"d_sum\", \n",
    "                                        ascending=False).head(no_max).index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress models for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "myModel = open(PATH_EXPORT_FOLDER + mdlFileName, 'rb')\n",
    "clf = joblib.load(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress\n",
    "#joblib.dump(clf, PATH_EXPORT_FOLDER + \\\n",
    "#    'mdl_cmp_RF_tags51_max_depthNone_max_features31_min_samples_split2_n_estimators25.pkl',\n",
    "#    compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from disk other useful tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CounterVectorizer\n",
    "tf_vectorizer_sup_1 = joblib.load(PATH_EXPORT_FOLDER + countVectFileName)\n",
    "# TfidfTransformer \n",
    "tfidf_transformer_sup_1 = joblib.load(PATH_EXPORT_FOLDER + tfidfFileName)\n",
    "# MultiLabelBinarizer\n",
    "mlb = joblib.load(PATH_EXPORT_FOLDER + mlbFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myTokenizer.__module__ = 'tagger_app'\n",
    "#str_save_cvect = PATH_EXPORT_FOLDER + \"cvect_tags51_ok.pkl\"\n",
    "#joblib.dump(tf_vectorizer_sup_1, str_save_cvect)\n",
    "#print(\"CountVectorizer Saved here: {}\".format(str_save_cvect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stopwords [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = joblib.load(PATH_EXPORT_FOLDER + stopWordsFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Test Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quest = pd.read_csv(PATH_SOURCE_QUESTIONS, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Text:\n",
      " freebsd newsyslog.conf.d set archive dir <p>On FreeBSD, I have a file named </p>\n",
      "\n",
      "<p><em>my_site</em></p>\n",
      "\n",
      "<p>in</p>\n",
      "\n",
      "<p><code>/usr/local/etc/newsyslog.conf.d</code></p>\n",
      "\n",
      "<p>The content i.e. the of file <code>my_site</code> looks like this:</p>\n",
      "\n",
      "<pre><code>/path/to/site/log/site.access_log 644 7 1048576 * GCZ /var/run/nginx.pid  30\n",
      "</code></pre>\n",
      "\n",
      "<p>Now I'd need to know, how I can specify the archive target directory.</p>\n",
      "\n",
      "<p>I haven't found anything in the docs how to set the archive dir in a config file.</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#id_question = id_supOK\n",
    "id_question = id_supNOK\n",
    "\n",
    "quest_text = df_quest[df_quest[\"Id\"] == id_question][\"Title\"] + \" \" + \\\n",
    "    df_quest[df_quest[\"Id\"] == id_question][\"Body\"]\n",
    "quest_text = quest_text.values[0]\n",
    "print(\"Question Text:\\n\", quest_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question cleaned:\n",
      " freebsd newsyslog conf d archive dir   freebsd   named       my site              usr local etc newsyslog conf d       content i e     my site   looks          path to site log site access log      gcz  var run nginx pid         d need know   specify archive target directory      n t found anything docs archive dir config file   \n"
     ]
    }
   ],
   "source": [
    "quest_text_cleaned = cleaning_text(quest_text)\n",
    "print(\"Question cleaned:\\n\", quest_text_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CounterVectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 18 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contVectValue = tf_vectorizer_sup_1.transform([quest_text_cleaned])\n",
    "contVectValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 18 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdfValue = tfidf_transformer_sup_1.transform(contVectValue)\n",
    "tfIdfValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_y_pred = clf.predict(tfIdfValue)\n",
    "encoded_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tu_tags = mlb.inverse_transform(encoded_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if tags found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag_tag_found = len(tu_tags[0]) > 0\n",
    "flag_tag_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If not, use unsupervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_tag_found == False:\n",
    "    # load\n",
    "    df_topics_tags = joblib.load(PATH_EXPORT_FOLDER + dfTopicsTagsFileName)\n",
    "    # load model\n",
    "    myModelUnsup = open(PATH_EXPORT_FOLDER + mdlUnsupFileName, 'rb')\n",
    "    model_lda = joblib.load(myModelUnsup)\n",
    "    # load count vect\n",
    "    tf_vectorizer_1 = joblib.load(PATH_EXPORT_FOLDER + countVectUnsupFileName)\n",
    "    # predict\n",
    "    tu_tags[0] = find_tags_from_text(text=quest_text, \n",
    "                    tf_vectorizer=tf_vectorizer_1,\n",
    "                    lda_model = model_lda, df_topics_tags=df_topics_tags,\n",
    "                    no_max=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python javascript java php '"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_out=\"\"\n",
    "for item in tu_tags[0]:\n",
    "    str_out += item + \" \"\n",
    "str_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# test 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
